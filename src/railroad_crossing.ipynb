{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitted-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded to open the port\n",
      "Succeeded to change the baudrate\n"
     ]
    }
   ],
   "source": [
    "# Importing servos and torch\n",
    "from SCSCtrl import TTLServo\n",
    "import torch\n",
    "# Setting camera little bit lower if it is not yet\n",
    "TTLServo.servoAngleCtrl(5,10,1,150)\n",
    "# Transfer the device from CPU memory to the GPU device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surprising-estimate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing TRT optimized models\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_road = TRTModule()\n",
    "model_road.load_state_dict(torch.load('best_steering_model_xy_trt.pth')) # well trained road following model\n",
    "\n",
    "model_collision = TRTModule()\n",
    "model_collision.load_state_dict(torch.load('best_model_trt.pth')) # well trained collision avoidance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amber-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from road_following/live_demo.ipynb\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "# Converting models to match camera\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-shopper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<traitlets.traitlets.directional_link at 0x7ef25b7be0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting camera, also its from jetbot own notebooks\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera()\n",
    "\n",
    "image_widget = ipywidgets.Image()\n",
    "\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inclusive-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing tobot to control motors\n",
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disturbed-newspaper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b3e8e187874514843f09979330630b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='speed gain', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba8b389d71445d19a46147f102410a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering gain', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b5ca973c1d409f9dadebe405dce0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering kd', max=0.5, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5b53677148415b91049102ad194fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering bias', max=0.3, min=-0.3, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Road following sliders\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01,  description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001,description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, description='steering bias')\n",
    "\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-ozone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9769302485f413d82084ccb380e5fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=0.0, description='stop', max=1.0, orientation='vertical'),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27a0b5a37504331bb7cfce6205cf3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Collision avoidance sliders\n",
    "stop_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, orientation='vertical',description='stop')\n",
    "\n",
    "display(ipywidgets.HBox([stop_slider]))\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forbidden-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all needed stuff\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from SCSCtrl import TTLServo\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import imutils\n",
    "import datetime\n",
    "\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "avg = None\n",
    "\n",
    "lastMovtionCaptured = datetime.datetime.now()\n",
    "\n",
    "def movement_detection(imageInput):\n",
    "    global avg, lastMovtionCaptured\n",
    "    servos = 0\n",
    "    # Current timestamp\n",
    "    timestamp = datetime.datetime.now()\n",
    "    # Image to gray color space\n",
    "    gray = cv.cvtColor(imageInput, cv.COLOR_BGR2GRAY)\n",
    "    # GaussionBlur helps camera to understand movement better\n",
    "    gray = cv.GaussianBlur(gray, (21, 21), 0)\n",
    "    # If the background has not been obtained, create a new one.\n",
    "    if avg is None:\n",
    "        avg = gray.copy().astype(\"float\")\n",
    "        return imageInput\n",
    "    # background update\n",
    "    cv.accumulateWeighted(gray, avg, 0.5)\n",
    "    # Comparing the difference between the new frame and the background.\n",
    "    frameDelta = cv.absdiff(gray, cv.convertScaleAbs(avg))\n",
    "    # Get the outline of the changed area in the frame.\n",
    "    thresh = cv.threshold(frameDelta, 5, 255,\n",
    "        cv.THRESH_BINARY)[1]\n",
    "    thresh = cv.dilate(thresh, None, iterations=2)\n",
    "    cnts_motion = cv.findContours(thresh.copy(), cv.RETR_EXTERNAL,\n",
    "        cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnts_motion = imutils.grab_contours(cnts_motion)\n",
    "    \n",
    "    for c in cnts_motion:\n",
    "        # smaller value means more sensitive movement, 800 is maximum value.\n",
    "            if cv.contourArea(c) > 100:\n",
    "                # Draw elements, including rectangle and text.\n",
    "                (mov_x, mov_y, mov_w, mov_h) = cv.boundingRect(c)\n",
    "                # Rectangle color is blue\n",
    "                cv.rectangle(imageInput, (mov_x, mov_y), (mov_x+mov_w, mov_y+mov_h), (255, 0, 0), 2)\n",
    "\n",
    "                # Save the current timestamp to mark the time when the change is detected.\n",
    "                lastMovtionCaptured = timestamp\n",
    "                # Servo does the movement\n",
    "                TTLServo.servoAngleCtrl(3, -30, 1, 500)\n",
    "                servos = 1\n",
    "            # if there is no movement then servo stays on normal position\n",
    "    if servos == 0:\n",
    "        TTLServo.servoAngleCtrl(3, 0, 1, 150)\n",
    "\n",
    "            \n",
    "    return imageInput\n",
    "\n",
    "def color_detection(imageInput):\n",
    "     # Image to hsv color space\n",
    "    hsv = cv.cvtColor(imageInput, cv.COLOR_BGR2HSV)\n",
    "    # Values for yellow, because train have yellow color and stop background dont have that color\n",
    "    colorUpperYellow = np.array([44, 255, 255])\n",
    "    colorLowerYellow = np.array([24, 100, 100])\n",
    "    # Values for blue, because train have blue color and stop background dont have that color\n",
    "    colorUpperBlue = np.array([140, 255, 255])\n",
    "    colorLowerBlue = np.array([100, 50, 50])\n",
    "    \n",
    "    # Mask for both colors\n",
    "    maskYellow = cv.inRange(hsv, colorLowerYellow, colorUpperYellow)\n",
    "    maskBlue = cv.inRange(hsv, colorLowerBlue, colorUpperBlue)\n",
    "    # Restoring bigger and smaller areas for both masks\n",
    "    maskYellow = cv.erode(maskYellow, None, iterations=2)\n",
    "    maskYellow = cv.dilate(maskYellow, None, iterations=2)\n",
    "    maskBlue = cv.erode(maskBlue, None, iterations=2)\n",
    "    maskBlue = cv.dilate(maskBlue, None, iterations=2)\n",
    "    # Creating one mask which contains both colors separately or together\n",
    "    mask = cv.bitwise_or(maskBlue, maskYellow)\n",
    "    # Obtaining the conformed area contour\n",
    "    cnts = cv.findContours(mask.copy(), cv.RETR_EXTERNAL,\n",
    "        cv.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    \n",
    "    if len(cnts) > 0:\n",
    "        # Finding the largest contour area\n",
    "        flag_area = max(cnts, key=cv.contourArea)\n",
    "        # Changing mask values to work better with image\n",
    "        mask = cv.bitwise_not(mask)\n",
    "        mask = cv.bitwise_and(imageInput,imageInput, mask=mask)\n",
    "        # Creating rectangle for the recognised color\n",
    "        (xg,yg,wg,hg) = cv.boundingRect(flag_area)\n",
    "        # Rectangle color is green\n",
    "        cv.rectangle(imageInput,(xg,yg),(xg+wg, yg+hg),(0,255,0),2)\n",
    "        # Servo does the movement\n",
    "        TTLServo.servoAngleCtrl(3, -30, 1, 500)\n",
    "    # If colors that train have are not found then servo is on normal state\n",
    "    else:\n",
    "        TTLServo.servoAngleCtrl(3, 0, 1, 150)\n",
    "        \n",
    "    return imageInput\n",
    "\n",
    "def railroad_crossing(imageInput):\n",
    "    # Global variables\n",
    "    global angle, angle_last, stop_slider, robot\n",
    "    global speed_value, steering_bias, steering_gain, steering_dgain\n",
    "    # Preprocessed road following model\n",
    "    xy = model_road(preprocess(imageInput)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    # Slider values for road following model\n",
    "    speed_value = speed_gain_slider.value\n",
    "    steering_bias = steering_bias_slider.value\n",
    "    steering_gain = steering_gain_slider.value\n",
    "    steering_dgain = steering_dgain_slider.value\n",
    "    # Preprocessed collision avoidance model\n",
    "    prob_stop = float(F.softmax(model_collision(preprocess(imageInput)), dim=1).flatten()[0])\n",
    "    # Slider value for stop from collision avoidance\n",
    "    stop_slider.value = prob_stop\n",
    "    \n",
    "    \n",
    "    # If the robot is in right place it stops and waits movement or right colors\n",
    "    # Thresshold value is 0.07 but you can change it higher to stop easier \n",
    "    if prob_stop < 0.07:\n",
    "        # All values to 0 because then robot dont move\n",
    "        robot.right_motor.value = 0.0\n",
    "        robot.right_motor.value = 0.0\n",
    "        x = 0.0\n",
    "        y = 0.0\n",
    "        speed_value = 0\n",
    "        steering_value = 0\n",
    "        steering_gain = 0\n",
    "        # Movement function, comment it if you dont want to use it\n",
    "        movement_detection(imageInput)\n",
    "        # Color function, comment it if you dont want to use it\n",
    "        color_detection(imageInput)\n",
    "\n",
    "\n",
    "    # Values from road following notebook to get robot move properly\n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain + (angle - angle_last) * steering_dgain\n",
    "    angle_last = angle\n",
    "    steering_value = pid + steering_bias\n",
    "\n",
    "    robot.left_motor.value = max(min(speed_value + steering_value, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_value - steering_value, 1.0), 0.0)\n",
    "\n",
    "\n",
    "    return imageInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "express-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing new camera frame\n",
    "def execute(change):\n",
    "    global image_widget\n",
    "    image = change['new']\n",
    "    image_widget.value = bgr8_to_jpeg(railroad_crossing(image))\n",
    "    \n",
    "execute({'new': camera.value})\n",
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "descending-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutting robot and camera down\n",
    "import time\n",
    "\n",
    "camera.unobserve(execute, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "robot.stop()\n",
    "camera.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
